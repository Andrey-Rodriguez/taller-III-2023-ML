[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Taller de Verano: 100 páginas de Machine Learning",
    "section": "",
    "text": "1 Taller de verano"
  },
  {
    "objectID": "1_introduccion.html",
    "href": "1_introduccion.html",
    "title": "2  Cómo funciona el aprendizaje supervisado",
    "section": "",
    "text": "Veremos el caso de las máquinas de soporte vectorial (SVM) para clasificación.\n\nPaso #1: Cargar librerías\n\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn import svm\nfrom sklearn.datasets import make_blobs\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\nfrom scipy.stats import distributions\nfrom numpy import sum\nimport numpy as np\n\n\nPaso #2: Crear datos\n\nSe crean 40 puntos usando la función make_blobs. Esta crea un conjunto de puntos separados en dos grupos.\n\nX, y = make_blobs(n_samples=40, centers=2, random_state=6)\n\n\nPaso #3: Crear el modelo\n\n\nclf = svm.SVC(kernel=\"linear\", C=1000)\n\n\nPaso #4: Entrenar el modelo\n\n\nclf.fit(X, y)\n\nSVC(C=1000, kernel='linear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVCSVC(C=1000, kernel='linear')\n\n\n\nPaso #5: Visualizar el modelo\n\n\nplt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)\n\n# plot the decision function\nax = plt.gca()\nDecisionBoundaryDisplay.from_estimator(\n    clf,\n    X,\n    plot_method=\"contour\",\n    colors=\"k\",\n    levels=[-1, 0, 1],\n    alpha=0.5,\n    linestyles=[\"--\", \"-\", \"--\"],\n    ax=ax,\n)\n# plot support vectors\nax.scatter(\n    clf.support_vectors_[:, 0],\n    clf.support_vectors_[:, 1],\n    s=100,\n    linewidth=1,\n    facecolors=\"none\",\n    edgecolors=\"k\",\n)\nplt.show()\n\n\n\n\n\nReferencias\n\nhttps://scikit-learn.org/stable/modules/svm.html#\nhttps://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane.html#sphx-glr-auto-examples-svm-plot-separating-hyperplane-py\n\n\n\n3 Estimación de parametros bayesiano\n\nalpha = 10\nbeta = 10\nn = 20\nNsamp = 201  # no of points to sample at\np = np.linspace(0, 1, Nsamp)\ndeltap = 1./(Nsamp-1)  # step size between samples of p\n\nprior = distributions.beta.pdf(p, alpha, beta)\n\nfor i in range(1, 9):\n\n    r = 2**i\n    n = (3.0/2.0)*r\n    like = distributions.binom.pmf(r, n, p)\n    like = like/(deltap*sum(like))  # for plotting convenience only\n    post = distributions.beta.pdf(p, alpha+r, beta+n-r)\n\n    # make the figure\n    plt.figure()\n    plt.plot(p, post, 'k', label='posterior')\n    plt.plot(p, like, 'r', label='likelihood')\n    plt.plot(p, prior, 'b', label='prior')\n    plt.xlabel('p')\n    plt.ylabel('PDF')\n    plt.legend(loc='best')\n    plt.title('r/n={}/{:.0f}'.format(r, n))\n    plt.show()"
  },
  {
    "objectID": "2_algoritmos_fundamentales.html#regresión-lineal",
    "href": "2_algoritmos_fundamentales.html#regresión-lineal",
    "title": "3  Día #2",
    "section": "3.1 Regresión Lineal",
    "text": "3.1 Regresión Lineal"
  },
  {
    "objectID": "2_algoritmos_fundamentales.html#regresión-logística",
    "href": "2_algoritmos_fundamentales.html#regresión-logística",
    "title": "3  Día #2",
    "section": "3.2 Regresión Logística",
    "text": "3.2 Regresión Logística"
  }
]