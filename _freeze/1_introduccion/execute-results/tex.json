{
  "hash": "3dd9a87794f3ec9c11f8f74eb571a13c",
  "result": {
    "markdown": "# Cómo funciona el aprendizaje supervisado \n\n\nVeremos el caso de las máquinas de soporte vectorial (SVM) para clasificación. \n\n\n- **Paso #1: Cargar librerías** \n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\nfrom sklearn import svm\nfrom sklearn.datasets import make_blobs\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\nfrom scipy.stats import distributions\nfrom numpy import sum\nimport numpy as np\n```\n:::\n\n\n- **Paso #2: Crear datos**\n\nSe crean 40 puntos usando la función `make_blobs`. Esta crea un conjunto de puntos separados en dos grupos. \n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nX, y = make_blobs(n_samples=40, centers=2, random_state=6)\n```\n:::\n\n\n- **Paso #3: Crear el modelo**\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nclf = svm.SVC(kernel=\"linear\", C=1000)\n```\n:::\n\n\n- **Paso #4: Entrenar el modelo**\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nclf.fit(X, y)\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```\nSVC(C=1000, kernel='linear')\n```\n:::\n:::\n\n\n- **Paso #5: Visualizar el modelo**\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nplt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)\n\n# plot the decision function\nax = plt.gca()\nDecisionBoundaryDisplay.from_estimator(\n    clf,\n    X,\n    plot_method=\"contour\",\n    colors=\"k\",\n    levels=[-1, 0, 1],\n    alpha=0.5,\n    linestyles=[\"--\", \"-\", \"--\"],\n    ax=ax,\n)\n# plot support vectors\nax.scatter(\n    clf.support_vectors_[:, 0],\n    clf.support_vectors_[:, 1],\n    s=100,\n    linewidth=1,\n    facecolors=\"none\",\n    edgecolors=\"k\",\n)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](1_introduccion_files/figure-pdf/cell-6-output-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n- **Referencias**\n\n  1. <https://scikit-learn.org/stable/modules/svm.html#>\n  2. <https://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane.html#sphx-glr-auto-examples-svm-plot-separating-hyperplane-py>\n \n# Estimación de parametros bayesiano\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nalpha = 10\nbeta = 10\nn = 20\nNsamp = 201  # no of points to sample at\np = np.linspace(0, 1, Nsamp)\ndeltap = 1./(Nsamp-1)  # step size between samples of p\n\nprior = distributions.beta.pdf(p, alpha, beta)\n\nfor i in range(1, 9):\n\n    r = 2**i\n    n = (3.0/2.0)*r\n    like = distributions.binom.pmf(r, n, p)\n    like = like/(deltap*sum(like))  # for plotting convenience only\n    post = distributions.beta.pdf(p, alpha+r, beta+n-r)\n\n    # make the figure\n    plt.figure()\n    plt.plot(p, post, 'k', label='posterior')\n    plt.plot(p, like, 'r', label='likelihood')\n    plt.plot(p, prior, 'b', label='prior')\n    plt.xlabel('p')\n    plt.ylabel('PDF')\n    plt.legend(loc='best')\n    plt.title('r/n={}/{:.0f}'.format(r, n))\n    plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](1_introduccion_files/figure-pdf/cell-7-output-1.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output .cell-output-display}\n![](1_introduccion_files/figure-pdf/cell-7-output-2.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output .cell-output-display}\n![](1_introduccion_files/figure-pdf/cell-7-output-3.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output .cell-output-display}\n![](1_introduccion_files/figure-pdf/cell-7-output-4.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output .cell-output-display}\n![](1_introduccion_files/figure-pdf/cell-7-output-5.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output .cell-output-display}\n![](1_introduccion_files/figure-pdf/cell-7-output-6.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output .cell-output-display}\n![](1_introduccion_files/figure-pdf/cell-7-output-7.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output .cell-output-display}\n![](1_introduccion_files/figure-pdf/cell-7-output-8.pdf){fig-pos='H'}\n:::\n:::\n\n\n",
    "supporting": [
      "1_introduccion_files/figure-pdf"
    ],
    "filters": []
  }
}